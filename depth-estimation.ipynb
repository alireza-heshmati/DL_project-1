{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T13:07:55.010086Z","iopub.execute_input":"2022-01-29T13:07:55.010469Z","iopub.status.idle":"2022-01-29T13:07:55.041548Z","shell.execute_reply.started":"2022-01-29T13:07:55.010338Z","shell.execute_reply":"2022-01-29T13:07:55.040522Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport timeit\nimport imp\nimport copy\nimport scipy.io as sci  # this is the SciPy module that loads mat-files\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, date, time\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:07:56.333188Z","iopub.execute_input":"2022-01-29T13:07:56.333837Z","iopub.status.idle":"2022-01-29T13:07:56.456770Z","shell.execute_reply.started":"2022-01-29T13:07:56.333801Z","shell.execute_reply":"2022-01-29T13:07:56.455679Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# TODO - NYU Depth V2:\n# The NYU-Depth V2 data set is comprised of video sequences from a variety of\n# indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft\n# Kinect. It features:\n#   - 1449 densely labeled pairs of aligned RGB and depth images\n#   - 464 new scenes taken from 3 cities\n#   - 407,024 new unlabeled frames\n#   - Each object is labeled with a class and an instance number (cup1, cup2, cup3, etc)\n# The dataset has several components:\n#   - Labeled: A subset of the video data accompanied by dense multi-class labels.\n#            This data has also been preprocessed to fill in missing depth labels.\n#   - Raw: The raw RGB, depth, and accelerometer data as provided by the Kinect.\n# Toolbox: Useful functions for manipulating the data and labels.\n\n\n\ndef downloader(link, base_path, file_name, zip=False):  \n    path = base_path + file_name\n    isdir = os.path.isdir(path[:-4])\n    if isdir:\n        print(file_name[:-4], \"folder already exists! Download skipped...\\n\")\n    else:\n        isFile = os.path.isfile(path)\n        if isFile:\n            print(file_name, \"already exists! Download skipped...\\n\")\n        else:\n            import requests\n            r = requests.get(link, stream = True)\n            # 1 MB = 1024 * 1024 byte\n            CS = 5000*1024*1024\n            with open(path, \"wb\") as file: \n                print(file_name, \"download started.\")\n                for i, block in enumerate(r.iter_content(chunk_size = CS)):\n                    # print(\".\", end = \"\")\n                    # print((i+1)*chunk_size,\"MB downloaded...\")\n                    file.write(block)\n                    # if block:\n                print(\"\\n\" + file_name, \"download finished.\")\n\nbase_path = \"./\"\nnyu_dep_name = \"nyu_depth_v2_labeled.mat\"\nnyu_dep_link = \"http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat\"\n\nstart= timeit.default_timer()\ndownloader(nyu_dep_link, base_path, nyu_dep_name)\n\nstop = timeit.default_timer()\nprint('Time (min): ', (stop - start)/60) \n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:07:56.729361Z","iopub.execute_input":"2022-01-29T13:07:56.730875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbase_path = \"./\"\nnyu_dep_name = \"nyu_depth_v2_labeled.mat\"\nnyu_dep_link = \"http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat\"\n\nnyu_dep_path = base_path + nyu_dep_name\n\nimport h5py\n\nf = h5py.File(nyu_dep_path,'r')\n\nnyu_dict = {}\n\nprint(\"Shapes:\\n\")\nfor key, value in f.items():\n    if key == 'depths' or key == 'images' :\n        nyu_dict[key] = np.array(value)\n        print(\"{:18s} {}\".format(key+\":\", nyu_dict[key].shape))\nprint(\"\\ndataset loaded!\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision.models\nimport collections\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n        \ndef conv(in_channels, out_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef depthwise(in_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,in_channels,kernel_size,stride=1,padding=padding,bias=False,groups=in_channels),\n          nn.BatchNorm2d(in_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef pointwise(in_channels, out_channels):\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,1,1,0,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MobileNet(nn.Module):\n    def __init__(self, relu6=True):\n        super(MobileNet, self).__init__()\n\n        def relu(relu6):\n            if relu6:\n                return nn.ReLU6(inplace=True)\n            else:\n                return nn.ReLU(inplace=True)\n\n        def conv_bn(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        def conv_dw(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                relu(relu6),\n    \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 2, relu6), \n            conv_dw( 32,  64, 1, relu6),\n            conv_dw( 64, 128, 2, relu6),\n            conv_dw(128, 128, 1, relu6),\n            conv_dw(128, 256, 2, relu6),\n            conv_dw(256, 256, 1, relu6),\n            conv_dw(256, 512, 2, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 1024, 2, relu6),\n            conv_dw(1024, 1024, 1, relu6),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(1024, 1000)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x\n\nprint('num parameters:', sum(p.numel() for p in MobileNet().parameters() if p.requires_grad))\nprint('\\n',MobileNet())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Depth_Estimator(nn.Module):\n    def __init__(self):\n        super(Depth_Estimator, self).__init__()\n        mobilenet = MobileNet()\n        # pretrained\n        pretrained_path = '../input/mobilenet-pretrained/mobilenet_model.pth.tar'\n        checkpoint = torch.load(pretrained_path)\n        state_dict = checkpoint['state_dict']\n\n        from collections import OrderedDict\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        mobilenet.load_state_dict(new_state_dict)\n\n\n        for i in range(14):\n            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n\n        # NNconv5\n        kernel_size = 5\n        self.decode_conv1 = nn.Sequential(\n            depthwise(1024, kernel_size),\n            pointwise(1024, 512))\n        self.decode_conv2 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 256))\n        self.decode_conv3 = nn.Sequential(\n            depthwise(256, kernel_size),\n            pointwise(256, 128))\n        self.decode_conv4 = nn.Sequential(\n            depthwise(128, kernel_size),\n            pointwise(128, 64))\n        self.decode_conv5 = nn.Sequential(\n            depthwise(64, kernel_size),\n            pointwise(64, 32))\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # encoder and preparing skip connections\n        for i in range(14):\n            layer = getattr(self, 'conv{}'.format(i))\n            x = layer(x)\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==1:\n                x1 = x\n            elif i==3:\n                x2 = x\n            elif i==5:\n                x3 = x\n                \n        # decoder with NNconv5 and additive skip connections\n        for i in range(1,6):\n            layer = getattr(self, 'decode_conv{}'.format(i))\n            x = layer(x)\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            if i==4:\n                x = x + x1\n            elif i==3:\n                x = x + x2\n            elif i==2:\n                x = x + x3\n            # print(\"{}: {}\".format(i, x.size()))\n        x = self.decode_conv6(x)\n        return x\n    \nModel_Estimator = Depth_Estimator()\nprint('\\n','num parameters:', sum(p.numel() for p in Model_Estimator.parameters() if p.requires_grad))\nprint(Model_Estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlen_index = len(nyu_dict['depths'])\nlist_index = np.arange(len(nyu_dict['depths']))\nnp.random.seed(0)\nnp.random.shuffle(list_index)\n\nclass preparing_data(torch.utils.data.Dataset):\n    def __init__(self, subset: str , index = list_index ,root = '../input/nyu-depth-v2/nyu_data' ):\n        \n        self.root = root\n        self.transform_rgb = transforms.Compose([ transforms.CenterCrop(336),transforms.Resize(224),\n                                                 transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n        \n        self.transform_t = transforms.Compose([transforms.CenterCrop(336),transforms.Resize(224)])\n        \n        len_ = len(index)\n        \n        if subset == \"train\":\n            self.datasets = index[:int(len_ * 0.8)]\n                \n        elif subset == \"validation\":\n            self.datasets = index[int(len_ * 0.7):int(len_ * 0.8)]\n        \n        elif  subset == \"test\":\n            self.datasets = index[int(len_ * 0.8):]\n    \n                \n        else :\n            raise ValueError(f\"Unknown subset {subset}. Use validation/testing/training\")\n            \n    def train_transform(self, data_, label_):\n        # perform 1st step of data augmentation\n        return  self.transform_rgb(data_) , self.transform_t(label_)\n\n    def __len__(self):\n        return len(self.datasets)\n    \n    def __getitem__(self,n):\n        \n        data = torch.tensor((nyu_dict['images'][0].transpose((0,2,1))),dtype = torch.float32)\n        label = torch.tensor((nyu_dict['depths'][0].transpose((1, 0))),dtype = torch.float32)[None, :]\n        data, label  = self.train_transform(data, label)\n\n        return data, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_validation(net, data_loader, device):\n    net.eval()\n    los = 0\n    for batch_idx, (data, target) in enumerate(data_loader):\n        data = data.to(device)\n        target = target.to(device)\n        output = net(data)\n        #los +=nn.MSELoss()(output, target)\n        los += nn.L1Loss()(output, target)\n    return np.round(los.cpu().detach().numpy() , 4) \n\ndef train_epoch(model, optimizer, train_loader, device):\n    model.train()\n    losses = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        output = model(data)\n        #loss =nn.MSELoss()(output, target)\n        loss = nn.L1Loss()(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses += loss\n\n    print(f\"Train Loss: {losses.item():.4f}\")\n\n    return losses\n\ndef run(model, train_loader, validation_loader, test_loader, optimizer, scheduler, device, checkpoint_file, n_epoch=10):\n    best_score = 0\n    best_model = copy.deepcopy(model)\n    \n    train_loss = []\n    val_loss = []\n    \n    for epoch in range(n_epoch):\n        print('\\n',f\"--- start epoch {epoch} ---\")\n        train_loss.append(train_epoch(model, optimizer, train_loader, device))\n        if scheduler:\n            scheduler.step()\n        val_loss.append(loss_validation(model, validation_loader, device))\n        print(f\"Validation_loss: {val_loss[-1]:.5f}\")\n        if best_score < val_loss[-1]:\n            best_score =  val_loss[-1]\n            best_model = copy.deepcopy(model)\n            torch.save(best_model.state_dict(), checkpoint_file)\n    print('\\n','************','\\n',f\"best validation_loss: {best_score:.5f}\")\n    test_loss = loss_validation(best_model, test_loader, device)\n    print(f\"best Test_loss: {test_loss:.5f}\")\n    \n    return train_loss, val_loss, test_loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_file = \"best_model\"\nbatch_size = 8 # 8\nepoch = 20 \n\noptimizer = 'sgd' # or adam\n\n# train\n\nif os.path.exists(checkpoint_file):\n    raise FileExistsError(f\"{checkpoint_file} already exists\")\n\nif device == \"cuda\":\n    num_workers = 1\n    pin_memory = True\nelse:\n    num_workers = 0\n    pin_memory = False\n\nprint(f\"Device: {device}\")\n\nmodel = Depth_Estimator().to(device)\n\ntrain_loader = torch.utils.data.DataLoader(\n    preparing_data(subset=\"train\"),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    pin_memory=pin_memory\n)\nvalidation_loader = torch.utils.data.DataLoader(\n    preparing_data(subset=\"validation\"),\n    batch_size=2,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n    pin_memory=pin_memory\n)\ntest_loader = torch.utils.data.DataLoader(\n    preparing_data(subset=\"test\"),\n    batch_size=2,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n    pin_memory=pin_memory\n)\n\nif optimizer == \"adam\":\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\nelif optimizer == \"sgd\":\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\nelse:\n    raise ValueError(f\"Unknown optimizer {optimizer}, use adam/sgd\")\n    \nstart = timeit.default_timer()\ntrain_loss, val_loss, test_loss = run(model,train_loader,validation_loader,test_loader,\n                                      optimizer,scheduler,device,checkpoint_file,n_epoch=epoch)\n\n\nprint('\\n','total Time (min) : ', round((timeit.default_timer()-start)/60, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}