{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-28T14:57:43.872959Z","iopub.execute_input":"2022-01-28T14:57:43.873227Z","iopub.status.idle":"2022-01-28T14:57:43.899594Z","shell.execute_reply.started":"2022-01-28T14:57:43.873150Z","shell.execute_reply":"2022-01-28T14:57:43.899038Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport timeit\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:49.964481Z","iopub.execute_input":"2022-01-28T14:57:49.964749Z","iopub.status.idle":"2022-01-28T14:57:49.968695Z","shell.execute_reply.started":"2022-01-28T14:57:49.964721Z","shell.execute_reply":"2022-01-28T14:57:49.967918Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"color = np.array(Image.open('../input/nyu-depth-v2/nyu_data/data/nyu2_test/00000_colors.png'))\ndepth = np.array(Image.open('../input/nyu-depth-v2/nyu_data/data/nyu2_test/00000_depth.png'))\ncolor.shape,depth.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:50.182558Z","iopub.execute_input":"2022-01-28T14:57:50.182883Z","iopub.status.idle":"2022-01-28T14:57:50.246248Z","shell.execute_reply.started":"2022-01-28T14:57:50.182853Z","shell.execute_reply":"2022-01-28T14:57:50.245583Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.imshow(color)\nplt.figure()\nplt.imshow(depth)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:50.621533Z","iopub.execute_input":"2022-01-28T14:57:50.622129Z","iopub.status.idle":"2022-01-28T14:57:51.134353Z","shell.execute_reply.started":"2022-01-28T14:57:50.622098Z","shell.execute_reply":"2022-01-28T14:57:51.133671Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"min(depth.flatten()),max(depth.flatten())","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:51.135698Z","iopub.execute_input":"2022-01-28T14:57:51.137001Z","iopub.status.idle":"2022-01-28T14:57:51.220366Z","shell.execute_reply.started":"2022-01-28T14:57:51.136959Z","shell.execute_reply":"2022-01-28T14:57:51.219726Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"depth","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:51.508112Z","iopub.execute_input":"2022-01-28T14:57:51.508322Z","iopub.status.idle":"2022-01-28T14:57:51.513776Z","shell.execute_reply.started":"2022-01-28T14:57:51.508296Z","shell.execute_reply":"2022-01-28T14:57:51.513145Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision.models\nimport collections\nimport math\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-01-28T15:31:35.446807Z","iopub.execute_input":"2022-01-28T15:31:35.447391Z","iopub.status.idle":"2022-01-28T15:31:35.452785Z","shell.execute_reply.started":"2022-01-28T15:31:35.447352Z","shell.execute_reply":"2022-01-28T15:31:35.451290Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:54.560049Z","iopub.execute_input":"2022-01-28T14:57:54.560290Z","iopub.status.idle":"2022-01-28T14:57:54.607811Z","shell.execute_reply.started":"2022-01-28T14:57:54.560255Z","shell.execute_reply":"2022-01-28T14:57:54.607127Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n        \ndef conv(in_channels, out_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef depthwise(in_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,in_channels,kernel_size,stride=1,padding=padding,bias=False,groups=in_channels),\n          nn.BatchNorm2d(in_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef pointwise(in_channels, out_channels):\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,1,1,0,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:55.162439Z","iopub.execute_input":"2022-01-28T14:57:55.164014Z","iopub.status.idle":"2022-01-28T14:57:55.176104Z","shell.execute_reply.started":"2022-01-28T14:57:55.163968Z","shell.execute_reply":"2022-01-28T14:57:55.175362Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MobileNet(nn.Module):\n    def __init__(self, relu6=True):\n        super(MobileNet, self).__init__()\n\n        def relu(relu6):\n            if relu6:\n                return nn.ReLU6(inplace=True)\n            else:\n                return nn.ReLU(inplace=True)\n\n        def conv_bn(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        def conv_dw(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                relu(relu6),\n    \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 2, relu6), \n            conv_dw( 32,  64, 1, relu6),\n            conv_dw( 64, 128, 2, relu6),\n            conv_dw(128, 128, 1, relu6),\n            conv_dw(128, 256, 2, relu6),\n            conv_dw(256, 256, 1, relu6),\n            conv_dw(256, 512, 2, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 1024, 2, relu6),\n            conv_dw(1024, 1024, 1, relu6),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(1024, 1000)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x\n\nprint('num parameters:', sum(p.numel() for p in MobileNet().parameters() if p.requires_grad))\nprint('\\n',MobileNet())","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:55.797728Z","iopub.execute_input":"2022-01-28T14:57:55.798264Z","iopub.status.idle":"2022-01-28T14:57:55.921999Z","shell.execute_reply.started":"2022-01-28T14:57:55.798225Z","shell.execute_reply":"2022-01-28T14:57:55.921290Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Depth_Estimator(nn.Module):\n    def __init__(self):\n        super(Depth_Estimator, self).__init__()\n        mobilenet = MobileNet()\n        # pretrained\n        pretrained_path = '../input/mobilenet-pretrained/mobilenet_model.pth.tar'\n        checkpoint = torch.load(pretrained_path)\n        state_dict = checkpoint['state_dict']\n\n        from collections import OrderedDict\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        mobilenet.load_state_dict(new_state_dict)\n\n\n        for i in range(14):\n            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n\n        # NNconv5\n        kernel_size = 5\n        self.decode_conv1 = nn.Sequential(\n            depthwise(1024, kernel_size),\n            pointwise(1024, 512))\n        self.decode_conv2 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 256))\n        self.decode_conv3 = nn.Sequential(\n            depthwise(256, kernel_size),\n            pointwise(256, 128))\n        self.decode_conv4 = nn.Sequential(\n            depthwise(128, kernel_size),\n            pointwise(128, 64))\n        self.decode_conv5 = nn.Sequential(\n            depthwise(64, kernel_size),\n            pointwise(64, 32))\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # encoder and preparing skip connections\n        for i in range(14):\n            layer = getattr(self, 'conv{}'.format(i))\n            x = layer(x)\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==1:\n                x1 = x\n            elif i==3:\n                x2 = x\n            elif i==5:\n                x3 = x\n                \n        # decoder with NNconv5 and additive skip connections\n        for i in range(1,6):\n            layer = getattr(self, 'decode_conv{}'.format(i))\n            x = layer(x)\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            if i==4:\n                x = x + x1\n            elif i==3:\n                x = x + x2\n            elif i==2:\n                x = x + x3\n            # print(\"{}: {}\".format(i, x.size()))\n        x = self.decode_conv6(x)\n        return x\n    \nModel_Estimator = Depth_Estimator()\nprint('\\n','num parameters:', sum(p.numel() for p in Model_Estimator.parameters() if p.requires_grad))\nprint(Model_Estimator)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:57.282826Z","iopub.execute_input":"2022-01-28T14:57:57.283357Z","iopub.status.idle":"2022-01-28T14:58:01.301372Z","shell.execute_reply.started":"2022-01-28T14:57:57.283317Z","shell.execute_reply":"2022-01-28T14:58:01.300632Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf_tr = pd.read_csv('../input/nyu-depth-v2/nyu_data/data/nyu2_train.csv', header=None)\ndf_tr.rename(columns = {0:'data', 1:'target'}, inplace = True)\ndf_test = pd.read_csv('../input/nyu-depth-v2/nyu_data/data/nyu2_test.csv', header=None)\ndf_test.rename(columns = {0:'data', 1:'target'}, inplace = True)\ndf_test = df_test.reset_index(drop=True)\n\nlen_tr = len(df_tr)\nlen_test = len(df_test)\n\ndf_tr['data'][0],df_tr['target'][0]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T15:23:52.906518Z","iopub.execute_input":"2022-01-28T15:23:52.906780Z","iopub.status.idle":"2022-01-28T15:23:52.993564Z","shell.execute_reply.started":"2022-01-28T15:23:52.906747Z","shell.execute_reply":"2022-01-28T15:23:52.992889Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_list = np.random.choice(len_tr,len_tr//30, replace=False)\nlen_ = len(train_list)\nind_shuffle = np.arange(len_)\nnp.random.shuffle(ind_shuffle)\ntrain_list = train_list[ind_shuffle]\n\ndf_train =df_tr.iloc[train_list[:len_//5]]\n\ndf_val =df_tr.iloc[train_list[len_//5:]]\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\n\nlen_train = len(df_train)\nlen_val = len(df_val)\nlen_train,len_val,len_test","metadata":{"execution":{"iopub.status.busy":"2022-01-28T15:23:53.842585Z","iopub.execute_input":"2022-01-28T15:23:53.843113Z","iopub.status.idle":"2022-01-28T15:23:53.856271Z","shell.execute_reply.started":"2022-01-28T15:23:53.843075Z","shell.execute_reply":"2022-01-28T15:23:53.855522Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class preparing_data(torch.utils.data.Dataset):\n    def __init__(self, subset: str ,root = '../input/nyu-depth-v2/nyu_data' ):\n\n        self.root = root\n        self.transform = transforms.Compose([\n            transforms.CenterCrop((336, 448)),\n            transforms.Resize(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n        if subset == \"train\":\n            self.datasets = df_train\n                \n        elif subset == \"validation\":\n            self.datasets = df_val\n        \n        elif  subset == \"test\":\n            self.datasets = df_test\n    \n                \n        else :\n            raise ValueError(f\"Unknown subset {subset}. Use validation/testing/training\")\n            \n    def train_transform(self, data_, label_):\n        # perform 1st step of data augmentation\n        return  self.transform(label_)\n\n    def __len__(self):\n        return len(self.datasets)\n    \n    def __getitem__(self,n):\n        \n        data = Image.open(os.path.join(self.root, self.datasets['data'][n])) \n        label = Image.open(os.path.join(self.root, self.datasets['target'][n])) \n        data, label  = self.train_transform(data, label)\n\n        return data, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = preparing_data('train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}