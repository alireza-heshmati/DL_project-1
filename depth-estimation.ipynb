{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T07:18:44.884041Z","iopub.execute_input":"2022-01-29T07:18:44.884613Z","iopub.status.idle":"2022-01-29T07:18:44.889867Z","shell.execute_reply.started":"2022-01-29T07:18:44.884572Z","shell.execute_reply":"2022-01-29T07:18:44.888705Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# TODO - NYU Depth V2:\n# The NYU-Depth V2 data set is comprised of video sequences from a variety of\n# indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft\n# Kinect. It features:\n#   - 1449 densely labeled pairs of aligned RGB and depth images\n#   - 464 new scenes taken from 3 cities\n#   - 407,024 new unlabeled frames\n#   - Each object is labeled with a class and an instance number (cup1, cup2, cup3, etc)\n# The dataset has several components:\n#   - Labeled: A subset of the video data accompanied by dense multi-class labels.\n#            This data has also been preprocessed to fill in missing depth labels.\n#   - Raw: The raw RGB, depth, and accelerometer data as provided by the Kinect.\n# Toolbox: Useful functions for manipulating the data and labels.\n\nimport imp\nimport numpy as np\nimport scipy.io as sci  # this is the SciPy module that loads mat-files\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, date, time\nimport pandas as pd\nimport timeit\n\ndef downloader(link, base_path, file_name, zip=False):  \n    path = base_path + file_name\n    isdir = os.path.isdir(path[:-4])\n    if isdir:\n        print(file_name[:-4], \"folder already exists! Download skipped...\\n\")\n    else:\n        isFile = os.path.isfile(path)\n        if isFile:\n            print(file_name, \"already exists! Download skipped...\\n\")\n        else:\n            import requests\n            r = requests.get(link, stream = True)\n            # 1 MB = 1024 * 1024 byte\n            CS = 5000*1024*1024\n            with open(path, \"wb\") as file: \n                print(file_name, \"download started.\")\n                for i, block in enumerate(r.iter_content(chunk_size = CS)):\n                    # print(\".\", end = \"\")\n                    # print((i+1)*chunk_size,\"MB downloaded...\")\n                    file.write(block)\n                    # if block:\n                print(\"\\n\" + file_name, \"download finished.\")\n\nbase_path = \"./\"\nnyu_dep_name = \"nyu_depth_v2_labeled.mat\"\nnyu_dep_link = \"http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat\"\n\nstart= timeit.default_timer()\ndownloader(nyu_dep_link, base_path, nyu_dep_name)\n\nstop = timeit.default_timer()\nprint('Time (min): ', (stop - start)/60) \n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T07:18:52.586635Z","iopub.execute_input":"2022-01-29T07:18:52.587647Z","iopub.status.idle":"2022-01-29T07:22:49.710751Z","shell.execute_reply.started":"2022-01-29T07:18:52.587608Z","shell.execute_reply":"2022-01-29T07:22:49.709606Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"nyu_depth_v2_labeled.mat download started.\n\nnyu_depth_v2_labeled.mat download finished.\nTime (min):  3.949628050866666\n","output_type":"stream"}]},{"cell_type":"code","source":"\nbase_path = \"./\"\nnyu_dep_name = \"nyu_depth_v2_labeled.mat\"\nnyu_dep_link = \"http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat\"\n\nnyu_dep_path = base_path + nyu_dep_name\n\nimport h5py\n\nf = h5py.File(nyu_dep_path,'r')\nnyu_dict = {}\n\nprint(\"Shapes:\\n\")\nfor key, value in f.items():\n    if key == 'depths' or key == 'images' :\n        nyu_dict[key] = np.array(value)\n        print(\"{:18s} {}\".format(key+\":\", nyu_dict[key].shape))\nprint(\"\\ndataset loaded!\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T07:13:32.910986Z","iopub.execute_input":"2022-01-29T07:13:32.911464Z","iopub.status.idle":"2022-01-29T07:13:33.228727Z","shell.execute_reply.started":"2022-01-29T07:13:32.911404Z","shell.execute_reply":"2022-01-29T07:13:33.227497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport timeit","metadata":{"execution":{"iopub.status.busy":"2022-01-29T06:55:15.296698Z","iopub.execute_input":"2022-01-29T06:55:15.297059Z","iopub.status.idle":"2022-01-29T06:55:15.301647Z","shell.execute_reply.started":"2022-01-29T06:55:15.297025Z","shell.execute_reply":"2022-01-29T06:55:15.300817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.imshow(nyu_dict['images'][2].transpose((2, 1, 0)))\nplt.figure()\nplt.imshow(nyu_dict['depths'][2].transpose((1, 0)),cmap='gray')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T07:02:13.89717Z","iopub.execute_input":"2022-01-29T07:02:13.89764Z","iopub.status.idle":"2022-01-29T07:02:14.605963Z","shell.execute_reply.started":"2022-01-29T07:02:13.897605Z","shell.execute_reply":"2022-01-29T07:02:14.605004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min(nyu_dict['depths'][0].flatten()),max(nyu_dict['depths'][0].flatten())","metadata":{"execution":{"iopub.status.busy":"2022-01-29T06:53:20.406797Z","iopub.execute_input":"2022-01-29T06:53:20.407161Z","iopub.status.idle":"2022-01-29T06:53:20.504225Z","shell.execute_reply.started":"2022-01-29T06:53:20.407125Z","shell.execute_reply":"2022-01-29T06:53:20.503272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.models\nimport collections\nimport math\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport time\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T06:55:31.35899Z","iopub.execute_input":"2022-01-29T06:55:31.359323Z","iopub.status.idle":"2022-01-29T06:55:32.778758Z","shell.execute_reply.started":"2022-01-29T06:55:31.359288Z","shell.execute_reply":"2022-01-29T06:55:32.777755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T06:56:29.44589Z","iopub.execute_input":"2022-01-29T06:56:29.44629Z","iopub.status.idle":"2022-01-29T06:56:29.452787Z","shell.execute_reply.started":"2022-01-29T06:56:29.446251Z","shell.execute_reply":"2022-01-29T06:56:29.451548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n        \ndef conv(in_channels, out_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef depthwise(in_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,in_channels,kernel_size,stride=1,padding=padding,bias=False,groups=in_channels),\n          nn.BatchNorm2d(in_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef pointwise(in_channels, out_channels):\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,1,1,0,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:55.162439Z","iopub.execute_input":"2022-01-28T14:57:55.164014Z","iopub.status.idle":"2022-01-28T14:57:55.176104Z","shell.execute_reply.started":"2022-01-28T14:57:55.163968Z","shell.execute_reply":"2022-01-28T14:57:55.175362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MobileNet(nn.Module):\n    def __init__(self, relu6=True):\n        super(MobileNet, self).__init__()\n\n        def relu(relu6):\n            if relu6:\n                return nn.ReLU6(inplace=True)\n            else:\n                return nn.ReLU(inplace=True)\n\n        def conv_bn(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        def conv_dw(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                relu(relu6),\n    \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 2, relu6), \n            conv_dw( 32,  64, 1, relu6),\n            conv_dw( 64, 128, 2, relu6),\n            conv_dw(128, 128, 1, relu6),\n            conv_dw(128, 256, 2, relu6),\n            conv_dw(256, 256, 1, relu6),\n            conv_dw(256, 512, 2, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 1024, 2, relu6),\n            conv_dw(1024, 1024, 1, relu6),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(1024, 1000)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x\n\nprint('num parameters:', sum(p.numel() for p in MobileNet().parameters() if p.requires_grad))\nprint('\\n',MobileNet())","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:55.797728Z","iopub.execute_input":"2022-01-28T14:57:55.798264Z","iopub.status.idle":"2022-01-28T14:57:55.921999Z","shell.execute_reply.started":"2022-01-28T14:57:55.798225Z","shell.execute_reply":"2022-01-28T14:57:55.92129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Depth_Estimator(nn.Module):\n    def __init__(self):\n        super(Depth_Estimator, self).__init__()\n        mobilenet = MobileNet()\n        # pretrained\n        pretrained_path = '../input/mobilenet-pretrained/mobilenet_model.pth.tar'\n        checkpoint = torch.load(pretrained_path)\n        state_dict = checkpoint['state_dict']\n\n        from collections import OrderedDict\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n        mobilenet.load_state_dict(new_state_dict)\n\n\n        for i in range(14):\n            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n\n        # NNconv5\n        kernel_size = 5\n        self.decode_conv1 = nn.Sequential(\n            depthwise(1024, kernel_size),\n            pointwise(1024, 512))\n        self.decode_conv2 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 256))\n        self.decode_conv3 = nn.Sequential(\n            depthwise(256, kernel_size),\n            pointwise(256, 128))\n        self.decode_conv4 = nn.Sequential(\n            depthwise(128, kernel_size),\n            pointwise(128, 64))\n        self.decode_conv5 = nn.Sequential(\n            depthwise(64, kernel_size),\n            pointwise(64, 32))\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # encoder and preparing skip connections\n        for i in range(14):\n            layer = getattr(self, 'conv{}'.format(i))\n            x = layer(x)\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==1:\n                x1 = x\n            elif i==3:\n                x2 = x\n            elif i==5:\n                x3 = x\n                \n        # decoder with NNconv5 and additive skip connections\n        for i in range(1,6):\n            layer = getattr(self, 'decode_conv{}'.format(i))\n            x = layer(x)\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            if i==4:\n                x = x + x1\n            elif i==3:\n                x = x + x2\n            elif i==2:\n                x = x + x3\n            # print(\"{}: {}\".format(i, x.size()))\n        x = self.decode_conv6(x)\n        return x\n    \nModel_Estimator = Depth_Estimator()\nprint('\\n','num parameters:', sum(p.numel() for p in Model_Estimator.parameters() if p.requires_grad))\nprint(Model_Estimator)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T14:57:57.282826Z","iopub.execute_input":"2022-01-28T14:57:57.283357Z","iopub.status.idle":"2022-01-28T14:58:01.301372Z","shell.execute_reply.started":"2022-01-28T14:57:57.283317Z","shell.execute_reply":"2022-01-28T14:58:01.300632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_rgb = transforms.Compose([\n    transforms.Resize(224),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n\na = transform_rgb(torch.tensor(nyu_dict['images'][0].transpose((2, 1, 0))/255))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T07:11:55.701098Z","iopub.execute_input":"2022-01-29T07:11:55.701476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class preparing_data(torch.utils.data.Dataset):\n    def __init__(self, subset: str ,root = '../input/nyu-depth-v2/nyu_data' ):\n\n        self.root = root\n        self.transform_rgb = transforms.Compose([\n            transforms.Resize(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n        \n        self.transform_t = transforms.Compose([\n            transforms.Resize(224)])\n        \n        len_ = len(nyu_dict['depths'])\n        list_ = np.arange(len(nyu_dict['depths']))\n        np.random.shuffle(list_)\n        \n        if subset == \"train\":\n            self.datasets = list_[:int(len_ * 0.7)]\n                \n        elif subset == \"validation\":\n            self.datasets = list_[int(len_ * 0.7):int(len_ * 0.8)]\n        \n        elif  subset == \"test\":\n            self.datasets = list_[int(len_ * 0.8):]\n    \n                \n        else :\n            raise ValueError(f\"Unknown subset {subset}. Use validation/testing/training\")\n            \n    def train_transform(self, data_, label_):\n        # perform 1st step of data augmentation\n        return  data_, self.transform_t(label_)\n\n    def __len__(self):\n        return len(self.datasets)\n    \n    def __getitem__(self,n):\n\n        data, label  = self.train_transform(data, label)\n\n        return data, label","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:59:46.836185Z","iopub.execute_input":"2022-01-28T17:59:46.83674Z","iopub.status.idle":"2022-01-28T17:59:46.847729Z","shell.execute_reply.started":"2022-01-28T17:59:46.836702Z","shell.execute_reply":"2022-01-28T17:59:46.846942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = preparing_data('train')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:59:48.072076Z","iopub.execute_input":"2022-01-28T17:59:48.072372Z","iopub.status.idle":"2022-01-28T17:59:48.079605Z","shell.execute_reply.started":"2022-01-28T17:59:48.072336Z","shell.execute_reply":"2022-01-28T17:59:48.078791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T18:00:42.457966Z","iopub.execute_input":"2022-01-28T18:00:42.458229Z","iopub.status.idle":"2022-01-28T18:00:42.634255Z","shell.execute_reply.started":"2022-01-28T18:00:42.458199Z","shell.execute_reply":"2022-01-28T18:00:42.633577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a[0][1]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T18:00:46.217996Z","iopub.execute_input":"2022-01-28T18:00:46.218698Z","iopub.status.idle":"2022-01-28T18:00:46.241109Z","shell.execute_reply.started":"2022-01-28T18:00:46.218659Z","shell.execute_reply":"2022-01-28T18:00:46.240469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(a[0][1])","metadata":{"execution":{"iopub.status.busy":"2022-01-28T18:01:01.877455Z","iopub.execute_input":"2022-01-28T18:01:01.878067Z","iopub.status.idle":"2022-01-28T18:01:01.898175Z","shell.execute_reply.started":"2022-01-28T18:01:01.878027Z","shell.execute_reply":"2022-01-28T18:01:01.897443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = Image.open('../input/nyu-depth-v2/nyu_data/data/nyu2_test/00008_depth.png') ","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:54:09.878752Z","iopub.execute_input":"2022-01-28T17:54:09.879027Z","iopub.status.idle":"2022-01-28T17:54:09.913072Z","shell.execute_reply.started":"2022-01-28T17:54:09.878994Z","shell.execute_reply":"2022-01-28T17:54:09.912411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(np.array(a).flatten())","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:54:12.844461Z","iopub.execute_input":"2022-01-28T17:54:12.844714Z","iopub.status.idle":"2022-01-28T17:54:12.896443Z","shell.execute_reply.started":"2022-01-28T17:54:12.844686Z","shell.execute_reply":"2022-01-28T17:54:12.895693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}